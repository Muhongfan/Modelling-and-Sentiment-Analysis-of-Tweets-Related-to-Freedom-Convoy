{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The codes have commented the ones thta applying the simple classifiers, as LR and NB, to the datasets with the pretrained word embeddings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "47aDT_rHBwOQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amber/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/amber/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amber/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/amber/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzNOaQPRBwOT",
    "outputId": "9a8f1f15-cba7-4094-ab8b-2028d19b3b75"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "U3YGPZggBwOW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('Freedom_Convoy.csv', index_col=0)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "uRSTWslsBwOX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     body sentiment_category\n0       The only comment I have is that people should ...                NEG\n1       Honestly I just wish everyone would stop givin...                NEG\n2       Yes. I’m starting to miss all those posts for ...                NEG\n3       Imagine creating new accounts to feed anti vac...                NEG\n4           I’m still good to complain about them, right?                NEU\n...                                                   ...                ...\n130152                            Just being neighbourly!                NEU\n130153  I just recently heard that “sunshine” from 302...                NEU\n130154  They are free to do this, its not like people ...                NEU\n130155  I hope some of them do so they get charged. Wi...                NEG\n130156                              Also actual pollution                NEG\n\n[130156 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>sentiment_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The only comment I have is that people should ...</td>\n      <td>NEG</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Honestly I just wish everyone would stop givin...</td>\n      <td>NEG</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Yes. I’m starting to miss all those posts for ...</td>\n      <td>NEG</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Imagine creating new accounts to feed anti vac...</td>\n      <td>NEG</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I’m still good to complain about them, right?</td>\n      <td>NEU</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>130152</th>\n      <td>Just being neighbourly!</td>\n      <td>NEU</td>\n    </tr>\n    <tr>\n      <th>130153</th>\n      <td>I just recently heard that “sunshine” from 302...</td>\n      <td>NEU</td>\n    </tr>\n    <tr>\n      <th>130154</th>\n      <td>They are free to do this, its not like people ...</td>\n      <td>NEU</td>\n    </tr>\n    <tr>\n      <th>130155</th>\n      <td>I hope some of them do so they get charged. Wi...</td>\n      <td>NEG</td>\n    </tr>\n    <tr>\n      <th>130156</th>\n      <td>Also actual pollution</td>\n      <td>NEG</td>\n    </tr>\n  </tbody>\n</table>\n<p>130156 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['body'].notnull()]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Data Cleaning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 130156 entries, 0 to 130156\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   body                130156 non-null  object\n",
      " 1   sentiment_category  130156 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='sentiment_category', ylabel='count'>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['sentiment_category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "count    130156.000000\nmean        130.901288\nstd         182.985155\nmin           7.000000\n25%          47.000000\n50%          84.000000\n75%         151.000000\nmax        9324.000000\nName: word_count, dtype: float64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_count'] = df['body'].apply(len)\n",
    "df['word_count'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['word_count'].hist(bins=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(df,col='sentiment_category')\n",
    "# g.map_dataframe(sns.histplot,'word_count', binrange=(0, 2000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='word numbers', ylabel='# of moments'>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1rElEQVR4nO3df1yV9f34/8fhHPAHIoRxOKhE6Wi2EuQ23UATJu5ISCgqWu2dJnPznSm+zcLUGlNu/lxM/LFVoLm5pW1pCsuzBMESN22WZUTlzJKEgoPyQ/mRHDhe3z/4cn1EOCAeDkg877fbbqPXuV7X8/XEc86T1/XjdWkURVEQQgghbpFTdw9ACCFEzyaFRAghhF2kkAghhLCLFBIhhBB2kUIihBDCLrruHkBXu3btGlarXKgmhBAd4eystflaryskVqtCZWVtdw9DCCF6FC8vN5uvyaEtIYQQdpFCIoQQwi5SSIQQQthFCokQQgi7SCERQghhFykkQggh7OLwQmK1WomJieF///d/AaisrCQuLo5JkyYRFxfH5cuX1W1TU1MxGo1ERERw7NgxtT0/P5/o6GiMRiNr1qyhacFii8XCkiVLMBqNzJw5k6KiIkenI4QQ4gYOLyR/+ctfGD58uPrfaWlphISEkJWVRUhICGlpaQCcO3cOk8mEyWRix44drF69GqvVCsCqVatISkoiKyuLgoICcnNzAdi7dy8DBw7k8OHDzJ07l+TkZEenI4QQ4gYOLSQlJSW8++67xMbGqm05OTnExMQAEBMTQ3Z2ttoeFRWFi4sLvr6++Pn5kZeXR2lpKdXV1QQFBaHRaIiJiSEnJweAI0eOMG3aNAAiIiI4ceIE8ngVIYToWg69s33dunUkJCRQU1OjtpWVlaHX6wHQ6/WUl5cDYDabCQwMVLfz9vbGbDaj0+kwGAxqu8FgwGw2q318fHwaE9HpcHNzo6KiAk9PT5tj0mo1eHj077wkb5FWY8VJ5+Kw/V9rsGBVbC9pIIQQncVhheSdd97B09OTBx54gP/85z/tbt/aTEKj0dhsb6tPW26XJVK8vNz4cttUh+1/eHwGZRerHLZ/IUTv0tYSKQ4rJB9++CFHjhwhNzeXuro6qqurefbZZxk0aBClpaXo9XpKS0vV2YPBYKCkpETtbzab0ev1LdpLSkrUGY3BYKC4uBiDwUBDQwNVVVV4eHg4KiUhhBCtcNg5kmeeeYbc3FyOHDnCpk2bCA4OJjk5mfDwcNLT0wFIT09n4sSJAISHh2MymbBYLBQWFlJQUEBAQAB6vR5XV1dOnz6Noigt+hw4cACAzMxMgoOD252RCCGE6Fxdvvrv/PnzWbJkCfv27cPHx4ctW7YA4O/vT2RkJJMnT0ar1ZKYmIhW23iMf9WqVaxYsYKrV68SGhpKaGgoALGxsSQkJGA0GnF3dyclJaWr0xFCiF5Po/Syy5zq66295hzJRTlHIoToJLKMvBBCCIeRQiKEEMIuUkiEEELYRQqJEEIIu0ghEUIIYRcpJEIIIewihUQIIYRdpJAIIYSwixQSIYQQdpFCIoQQwi5SSIQQQthFCokQQgi7SCERQghhFykkQggh7CKFRAghhF2kkAghhLCLFBIhhBB2cVghqaurIzY2lilTphAVFcXWrVsB2LZtG+PHj2fq1KlMnTqVo0ePqn1SU1MxGo1ERERw7NgxtT0/P5/o6GiMRiNr1qyh6aGOFouFJUuWYDQamTlzJkVFRY5KRwghhA0Oe2a7i4sLu3btwtXVlfr6en7xi1+oz1qfO3cu8+bNa7b9uXPnMJlMmEwmzGYzcXFxZGZmotVqWbVqFUlJSYwaNYpf//rX5ObmEhYWxt69exk4cCCHDx/GZDKRnJzM5s2bb3qMnu590bo4d2bazVgt9ZRfvuqw/QshxO3AYYVEo9Hg6uoKQENDAw0NDWg0Gpvb5+TkEBUVhYuLC76+vvj5+ZGXl8eQIUOorq4mKCgIgJiYGHJycggLC+PIkSMsWrQIgIiICJKSklAUpc0419O6OHPx5dfszNQ2rwWPA1JIhBDfbw4rJABWq5Xp06dz4cIFfvGLXxAYGEhubi67d+8mPT2dBx54gOXLl+Pu7o7ZbCYwMFDt6+3tjdlsRqfTYTAY1HaDwYDZbAbAbDbj4+PTmIhOh5ubGxUVFXh6etock1arwcOjv4MybqkrY91OsYUQvYdDC4lWqyUjI4MrV66wcOFCzp49y2OPPcZTTz2FRqNhy5YtbNiwgfXr16vnPa6n0WhstgNtvmaL1apQWVkLgJeX262k1SFNsW7UnbGFEKKj2vrO6pKrtgYOHMhPf/pTjh07xp133olWq8XJyYmZM2fyySefAI0zjZKSErWP2WxGr9e3aC8pKUGv16t9iouLgcbDZ1VVVXh4eHRFSkIIIf5/Disk5eXlXLlyBYCrV69y/Phxhg0bRmlpqbpNdnY2/v7+AISHh2MymbBYLBQWFlJQUEBAQAB6vR5XV1dOnz6Noiikp6czceJEtc+BAwcAyMzMJDg4+KbPjwghhOgcDju0VVpayvLly7FarSiKwkMPPcSECRNISEjgzJkzAAwZMoSkpCQA/P39iYyMZPLkyWi1WhITE9FqtQCsWrWKFStWcPXqVUJDQ9Wrv2JjY0lISMBoNOLu7k5KSoqj0hFCCGGDRmntRMP3WH29tdk5EkdftXXxYlXrr3m58eW2qQ6LPTw+w2ZsIYToqG4/RyKEEOL7SwqJEEIIu0ghEUIIYRcpJEIIIewihUQIIYRdpJAIIYSwixQSIYQQdpFCIoQQwi5SSIQQQthFCokQQgi7SCERQghhFykkQggh7CKFRAghhF2kkAghhLCLFBIhhBB2kUIihBDCLg4rJHV1dcTGxjJlyhSioqLYunUrAJWVlcTFxTFp0iTi4uK4fPmy2ic1NRWj0UhERATHjh1T2/Pz84mOjsZoNLJmzRqansVlsVhYsmQJRqORmTNnUlRU5Kh0hBBC2OCwQuLi4sKuXbv4xz/+QXp6OseOHeP06dOkpaUREhJCVlYWISEhpKWlAXDu3DlMJhMmk4kdO3awevVqrFYr0Pio3aSkJLKysigoKCA3NxeAvXv3MnDgQA4fPszcuXNJTk52VDpCCCFscFgh0Wg0uLq6AtDQ0EBDQwMajYacnBxiYmIAiImJITs7G4CcnByioqJwcXHB19cXPz8/8vLyKC0tpbq6mqCgIDQaDTExMeTk5ABw5MgRpk2bBkBERAQnTpyglz05WAghup1Dz5FYrVamTp3K2LFjGTt2LIGBgZSVlaHX6wHQ6/WUl5cDYDabMRgMal9vb2/MZnOLdoPBgNlsVvv4+PgAoNPpcHNzo6KiwpEpCSGEuIHOkTvXarVkZGRw5coVFi5cyNmzZ21u29pMQqPR2Gxvq0/bY9Lg4dG/vaF3mq6MdTvFFkL0Hg4tJE0GDhzIT3/6U44dO8agQYMoLS1Fr9dTWlqKp6cn0DjTKCkpUfuYzWb0en2L9pKSEnVGYzAYKC4uxmAw0NDQQFVVFR4eHm2OxWpVqKysBcDLy62TM22pKdaNujO2EEJ0VFvfWQ47tFVeXs6VK1cAuHr1KsePH2fYsGGEh4eTnp4OQHp6OhMnTgQgPDwck8mExWKhsLCQgoICAgIC0Ov1uLq6cvr0aRRFadHnwIEDAGRmZhIcHNzujEQIIUTnctiMpLS0lOXLl2O1WlEUhYceeogJEyYwatQolixZwr59+/Dx8WHLli0A+Pv7ExkZyeTJk9FqtSQmJqLVaoHGq7ZWrFjB1atXCQ0NJTQ0FIDY2FgSEhIwGo24u7uTkpLiqHSEEELYoFF62WVO9fXWZoe2Lr78msNieS14nIsXq1p/zcuNL7dNdVjs4fEZNmMLIURHdcuhLSGEEL2DFBIhhBB2kUIihBDCLlJIhBBC2EUKiRBCCLtIIRFCCGEXKSRCCCHs0m4hOXXqFLW1jfddZGRksH79er755huHD0wIIUTP0G4hWbVqFf369ePMmTPs2LGDwYMH89xzz3XF2IQQQvQA7RYSnU6HRqMhOzubOXPm8MQTT1BTU9MVYxNCCNEDtFtIXF1dSU1N5a233uJnP/sZVquVhoaGrhibEEKIHqDdQpKSkoKLiwtr167Fy8sLs9nMvHnzumJsQggheoB2V//985//TEJCgvrfgwcP5osvvnDooIQQQvQc7c5Ijh8/3qItNzfXIYMRQgjR89ickezZs4fXX3+dwsJCoqOj1faamhqCgoK6ZHBCCCFufzYLSXR0NKGhoWzatIlnnnlGbXd1dW33cbZCCCF6D5uFxM3NDTc3NzZt2oTVauXSpUtYrVZqa2upra1l8ODBXTlOIYQQt6l2T7a/9tprbNu2jTvvvBMnp/93SuWtt95qs19xcTHLli3j0qVLODk5MWvWLJ544gm2bdvGG2+8gaenJwBLly4lLCwMgNTUVPbt24eTkxMvvPAC48ePByA/P1991G5YWBjPP/88Go0Gi8XCsmXL+PTTT/Hw8CAlJYWhQ4fe8i9DCCFEx7VbSHbt2sWhQ4e44447OrRjrVbL8uXLuf/++6murmbGjBmMGzcOgLlz57a4hPjcuXOYTCZMJhNms5m4uDgyMzPRarWsWrWKpKQkRo0axa9//Wtyc3MJCwtj7969DBw4kMOHD2MymUhOTmbz5s0dGqcQQgj7tHvVlsFgwM3N9rN6bdHr9dx///0ADBgwgGHDhmE2m21un5OTQ1RUFC4uLvj6+uLn50deXh6lpaVUV1cTFBSERqMhJiaGnJwcAI4cOcK0adMAiIiI4MSJE/SyR9ALIUS3a3dG4uvry+zZs/nZz36Gi4uL2h4XF3fTQYqKivj8888JDAzkww8/ZPfu3aSnp/PAAw+wfPly3N3dMZvNBAYGqn28vb0xm83odDoMBoPabjAY1IJkNpvx8fFpTESnw83NjYqKCvWwmRBCCMdrt5AMHjyYwYMHU19fT319fYcD1NTUsHjxYlauXMmAAQN47LHHeOqpp9BoNGzZsoUNGzawfv36VmcSGo3GZjvQ5mu2aLUaPDz6dziPW9WVsW6n2EKI3qPdQrJo0SIAamtr6d+/Y19M9fX1LF68mOjoaCZNmgTAnXfeqb4+c+ZMnnzySaBxplFSUqK+Zjab0ev1LdpLSkrQ6/Vqn+LiYgwGAw0NDVRVVbV7abLVqlBZ2bgsvpdXxw/ZdVRTrBt1Z2whhOiotr6z2j1H8tFHHzF58mQmT54MwJkzZ1i1alW7QRVF4fnnn2fYsGHNDoOVlpaqP2dnZ+Pv7w9AeHg4JpMJi8VCYWEhBQUFBAQEoNfrcXV15fTp0yiKQnp6OhMnTlT7HDhwAIDMzEyCg4PbnZEIIYToXO3OSNatW8err77KggULABgxYgQffPBBuzs+deoUGRkZ3HvvvUydOhVovNT34MGDnDlzBoAhQ4aQlJQEgL+/P5GRkUyePBmtVktiYiJarRZofCZK0+W/oaGhhIaGAhAbG0tCQgJGoxF3d3dSUlJu4VcghBDCHu0WEkA9od3k+vtJbBk9ejT//e9/W7Q33TPSmgULFqgF63ojR47k4MGDLdr79OnD1q1b2x2LEEIIx2m3kPj4+PDhhx+qNwD+9a9/Zfjw4V0xNiGEED3ATT1qd/fu3ZjNZsLCwvj8889JTEzsirEJIYToAdqdkXh6evL73/++K8YihBCiB2q3kBQWFvLaa6/xzTffNHvE7iuvvOLQgQkhhOgZ2i0kCxcuJDY2lgkTJtzUSXYhhBC9S7uFpE+fPsyZM6crxiKEEKIHareQzJkzhz/84Q+MGzeu2VpbTQsyCiGE6N3aLSRnz54lIyOD9957T71rXKPR8Je//MXhgxNCCHH7a7eQHD58mOzs7GazESGEEKJJu2fPR4wYQVVVVVeMRQghRA/U7oykrKyMyMhIRo4cibOzs9oul/8KIYSAmygk8fHxXTEO0UU83F1wdunjsP3XW+qovGxx2P6FELefdgvJT37yEy5dusQnn3wCQEBAAIMGDXL4wIRjOLv04eDOSIft/+Ffvg1IIRGiN2n3HMk///lPZs6cyaFDh3j77bfVn4UQQgi4iRnJK6+8wr59+9RZSHl5OXPnzuWhhx5y+OCEEELc/tqdkSiK0uxQloeHR6vPShdCCNE7tTsjefDBB5k3bx5RUVFA46Gu8ePHO3xgQggheoZ2ZyTPPfccs2bN4r///S9nzpzhkUceYdmyZe3uuLi4mNmzZxMZGUlUVBS7du0CoLKykri4OCZNmkRcXByXL19W+6SmpmI0GomIiODYsWNqe35+PtHR0RiNRtasWaPOiCwWC0uWLMFoNDJz5kyKioo6/AsQQghhn5tazjciIoL4+HieeuopxowZQ2VlZbt9tFoty5cv5+233+bvf/87e/bs4dy5c6SlpRESEkJWVhYhISGkpaUBcO7cOUwmEyaTiR07drB69WqsVivQ+HCtpKQksrKyKCgoIDc3F4C9e/cycOBADh8+zNy5c0lOTr7FX4MQQohb1W4h+dvf/sbYsWOZMmUKM2bMYPr06cyYMaPdHev1enVhxwEDBjBs2DDMZjM5OTnExMQAEBMTQ3Z2NgA5OTlERUXh4uKCr68vfn5+5OXlUVpaSnV1NUFBQWg0GmJiYsjJyQHgyJEjTJs2DWgsdidOnJDzN0II0cXaPUeyc+dODh48iKen5y0HKSoq4vPPPycwMJCysjL0ej3QWGzKy8sBMJvNBAYGqn28vb0xm83odDoMBoPabjAYMJvNah8fH5/GRHQ63NzcqKioaHOsWq0GD4/+t5xLR3VlrNsldnfmLIToeu0WEl9fX/r163fLAWpqali8eDErV65kwIABNrdrbSah0WhstrfVpy1Wq0JlZS0AXl5ubW7bGZpi3ai7YndnzkKInqut7452C8kzzzzDo48+SmBgYLMVgF944YV2A9fX17N48WKio6OZNGkSAIMGDaK0tBS9Xk9paak6ezAYDJSUlKh9zWYzer2+RXtJSYk6ozEYDBQXF2MwGGhoaKCqqgoPD492xyWEEKLztHuOJDExkeDgYAIDA7n//vvV/7VHURSef/55hg0bRlxcnNoeHh5Oeno6AOnp6UycOFFtN5lMWCwWCgsLKSgoICAgAL1ej6urK6dPn0ZRlBZ9Dhw4AEBmZibBwcHtzkiEEEJ0rnZnJDqdjhUrVnR4x6dOnSIjI4N7772XqVOnArB06VLmz5/PkiVL2LdvHz4+PmzZsgUAf39/IiMjmTx5MlqtlsTERLRaLdB41daKFSu4evUqoaGhhIaGAhAbG0tCQgJGoxF3d3dSUlI6PE4hhBD20SjtXOaUkpLC4MGDmTBhQrNDWz31EFJ9vbXZOZKLL7/msFheCx7n4sXWn+Xi5eXGl9umOiz28PiMVmN7ebk5fNFGWzkLIXouu86RvPXWW0DjzYJNNBqNegmuEEKI3q3dQnLkyJGuGIcQQoge6qbubBdCCCFskUIihBDCLjYLyalTp4DGhRGFEEIIW2wWkrVr1wLwyCOPdNlghBBC9Dw2T7Y33T9iNptZs2ZNi9dv5s52IYQQ3382C8krr7zCiRMneO+9927qTnYhhBC9k81C4unpSVRUFMOHD2fEiBFdOSYhhBA9SLtXbXl4eLBw4UJCQkIYO3Ys8fHxzRZRFEII0bu1W0hWrFhBeHg4x44dIzc3lwkTJtzS2ltCCCG+n9otJGVlZcyYMQOdTodOp2P69Onqw6iEEEKIdguJp6cnGRkZWK1WrFYrGRkZPXbBRiGEEJ2v3UKybt063n77bcaNG8eDDz5IZmYm69at64qxCSGE6AHaXbRx8ODBvPLKK10xFiGEED2QrLUlhBDCLlJIhBBC2MVhhWTFihWEhITw8MMPq23btm1j/PjxTJ06lalTp3L06FH1tdTUVIxGIxERERw7dkxtz8/PJzo6GqPRyJo1a2h6oKPFYmHJkiUYjUZmzpxJUVGRo1IRQgjRhnYLyUsvvaT+3JGVgKdPn86OHTtatM+dO5eMjAwyMjIICwsD4Ny5c5hMJkwmEzt27GD16tVYrVag8XntSUlJZGVlUVBQQG5uLgB79+5l4MCBHD58mLlz55KcnHzTYxNCCNF5bBaS7du389FHH5GZmam2dWQl4DFjxuDu7n5T2+bk5BAVFYWLiwu+vr74+fmRl5dHaWkp1dXVBAUFodFoiImJUR/xe+TIEaZNmwZAREQEJ06coJ3HzwshhHAAm1dt3XPPPRw6dIjCwkJ+8YtfMHz4cCorK/nqq68YNmzYLQfcvXs36enpPPDAAyxfvhx3d3fMZjOBgYHqNt7e3pjNZnQ6HQaDQW03GAyYzWYAzGYzPj4+jUnodLi5uVFRUYGnp2eb8bVaDR4e/W95/B3VlbFul9jdmbMQouvZLCTu7u4sXbqUkydP8te//pUvv/ySf/3rX2zfvp3z58/zt7/9rcPBHnvsMZ566ik0Gg1btmxhw4YNrF+/vtWZhEajsdkOtPlaW6xWhcrKWgC8vNw6mkKHNcW6UXfF7s6chRA9V1vfHTYLybFjx/jDH/7AhQsXWL9+PSNGjKBfv36sX7/+lgdy5513qj/PnDmTJ598EmicaVy/EKTZbEav17doLykpQa/Xq32Ki4sxGAw0NDRQVVUld9wLIUQ3sHmOZOnSpezatYshQ4YwZcoUrFYr5eXlPPbYY2oB6KjS0lL15+zsbPz9/QEIDw/HZDJhsVgoLCykoKCAgIAA9Ho9rq6unD59GkVRSE9PZ+LEiWqfAwcOAJCZmUlwcPBNzUiEEEJ0rnbvbH/wwQcJCAggICCA119/nddff/2mFm1sOixWUVFBaGgo8fHxnDx5kjNnzgAwZMgQkpKSAPD39ycyMpLJkyej1WpJTExEq9UCjVdtrVixgqtXrxIaGkpoaCgAsbGxJCQkYDQacXd3JyUl5ZZ/CUIIIW6dRunApU5nzpzp8Q+5qq+3NjtHcvHl1xwWy2vB41y8WNX6a15ufLltqsNiD4/PaDW2l5cbB3dGOizuw79822bOQoieq61zJB26IbGnFxEhhBCdT5ZIEUIIYRcpJEIIIewihUQIIYRdpJAIIYSwixQSIYQQdpFCIoQQwi5SSIQQQthFCokQQgi7SCERQghhFykkQggh7NLuoo1CdAZ3D2dcnPs6NIal/iqXK+sdGkMI0ZIUEtElXJz7krInwqExnv5FJiCFRIiuJoe2hBBC2EUKiRBCCLtIIRFCCGEXKSRCCCHs4rBCsmLFCkJCQnj44YfVtsrKSuLi4pg0aRJxcXFcvnxZfS01NRWj0UhERATHjh1T2/Pz84mOjsZoNLJmzRqaHuhosVhYsmQJRqORmTNnUlRU5KhUhBBCtMFhhWT69Ons2LGjWVtaWhohISFkZWUREhJCWloaAOfOncNkMmEymdixYwerV6/GarUCjc9sT0pKIisri4KCAnJzcwHYu3cvAwcO5PDhw8ydO5fk5GRHpSKEEKINDiskY8aMwd3dvVlbTk4OMTExAMTExJCdna22R0VF4eLigq+vL35+fuTl5VFaWkp1dTVBQUFoNBpiYmLIyckB4MiRI0ybNg2AiIgITpw4QQcePy+EEKKTdOl9JGVlZej1egD0ej3l5eUAmM1mAgMD1e28vb0xm83odDoMBoPabjAYMJvNah8fHx8AdDodbm5uVFRU4Onp2eYYtFoNHh79OzWvtnRlrNsldm/MWYje7La4IbG1mYRGo7HZ3laf9litCpWVtQB4ebl1dKgd1hTrRt0V+/sc11ZsIYT92voMd+lVW4MGDaK0tBSA0tJSdfZgMBgoKSlRtzObzej1+hbtJSUl6ozGYDBQXFwMQENDA1VVVXh4eHRRJkIIIZp0aSEJDw8nPT0dgPT0dCZOnKi2m0wmLBYLhYWFFBQUEBAQgF6vx9XVldOnT6MoSos+Bw4cACAzM5Pg4OCbmpEIIYToXA47tLV06VJOnjxJRUUFoaGhxMfHM3/+fJYsWcK+ffvw8fFhy5YtAPj7+xMZGcnkyZPRarUkJiai1WqBxqu2VqxYwdWrVwkNDSU0NBSA2NhYEhISMBqNuLu7k5KS4qhUhBBCtMFhhWTTpk2ttu/atavV9gULFrBgwYIW7SNHjuTgwYMt2vv06cPWrVvtG6QQQgi7yZ3tQggh7CKFRAghhF2kkAghhLCLFBIhhBB2kUIihBDCLlJIhBBC2EUKiRBCCLtIIRFCCGEXKSRCCCHsIoVECCGEXaSQCCGEsMtt8TwSIRzJzaMPfZ1dHLb/q/UWqirrHLZ/IW53UkjE915fZxciM5502P7fnvoKVUghEb2XHNoSQghhFykkQggh7CKFRAghhF2kkAghhLBLt5xsDw8Px9XVFScnJ7RaLfv376eyspKnn36ab775hiFDhrB582bc3d0BSE1NZd++fTg5OfHCCy8wfvx4APLz89XH8IaFhfH888/Lc9uFEKKLdduMZNeuXWRkZLB//34A0tLSCAkJISsri5CQENLS0gA4d+4cJpMJk8nEjh07WL16NVarFWh8nntSUhJZWVkUFBSQm5vbXekIIUSvddsc2srJySEmJgaAmJgYsrOz1faoqChcXFzw9fXFz8+PvLw8SktLqa6uJigoCI1GQ0xMDDk5Od2YgRBC9E7ddh/JvHnz0Gg0PPLIIzzyyCOUlZWh1+sB0Ov1lJeXA2A2mwkMDFT7eXt7Yzab0el0GAwGtd1gMGA2m9uNq9Vq8PDo38nZ2NaVsW6X2JKzEL1LtxSS119/HW9vb8rKyoiLi2PYsGE2t1UUpUWbRqOx2d4eq1WhsrIWAC8vtw6M+tY0xbpRd8X+Psftzti2/p2F+L5o63PULYe2vL29ARg0aBBGo5G8vDwGDRpEaWkpAKWlpXh6egKNM42SkhK1r9lsRq/Xt2gvKSlRZzRCCCG6TpcXktraWqqrq9Wf//3vf+Pv7094eDjp6ekApKenM3HiRKDxCi+TyYTFYqGwsJCCggICAgLQ6/W4urpy+vRpFEVp1kcIIUTX6fJDW2VlZSxcuBAAq9XKww8/TGhoKCNHjmTJkiXs27cPHx8ftmzZAoC/vz+RkZFMnjwZrVZLYmIiWq0WaLxqq+ny39DQUEJDQ7s6HSFscvPoS19nZ4fGuFpfT1XlVYfGEKI9XV5IfH19+cc//tGi/Y477mDXrl2t9lmwYAELFixo0T5y5EgOHjzY6WMUojP0dXYm6s1Uh8YwzfhfqpBCIrrXbXP5rxBCiJ5JlpEXQogeyNO9P1oXrUNjWC1Wyi+3f0WiFBIhhOiBtC5azJtPOjSG95Kf3NR2cmhLCCGEXaSQCCGEsIsUEiGEEHaRcyRCfA+5efSjr7PjPt5X6xuoqvzOYfsXPYsUEiG+h/o665iyL8Nh+/9H7FSqHLZ30dPIoS0hhBB2kUIihBDCLnJoSwjRadw8+tPX2bE3yV2tt1Ily/bfVqSQCCE6TV9nLTPfzHdojL0zHpDzM7cZObQlhBDCLjIjEUJ8L3h4uOLs7Li/jevrr1FZWeOw/fdkUkiEEN8Lzs5OvPHmJYftf9aMOx22755OCokQQtjhDndXdC6Omwk1WK5Rcfn2nglJIRFCCDvoXJw485LZYfsf8ZS3w/bdWXr8yfbc3FwiIiIwGo2kpaV193CEEKLX6dGFxGq1kpSUxI4dOzCZTBw8eJBz585197CEEKJX6dGFJC8vDz8/P3x9fXFxcSEqKoqcnJzuHpYQQvQqGkVRlO4exK06dOgQx44dY+3atQCkp6eTl5dHYmJiN49MCCF6jx49I2mtBmo0mm4YiRBC9F49upAYDAZKSkrU/zabzej1+m4ckRBC9D49upCMHDmSgoICCgsLsVgsmEwmwsPDu3tYQgjRq/To+0h0Oh2JiYn86le/wmq1MmPGDPz9/bt7WEII0av06JPtQgghul+PPrQlhBCi+0khEUIIYZcefY6kM+Xm5rJ27VquXbvGzJkzmT9/fqvbNZ2L8fb2JjU1FYAzZ87w29/+ltraWoYMGUJycjIDBgywGSs8PBxXV1ecnJzQarXs37+fyspKnn76ab755huGDBnC5s2bcXd3b9HXViyLxcJvf/tbsrOzqampwWAwcOTIEYA2952amsq+fftwcnLihRdeYPz48S1iVlRUsHjxYvLz85k2bVqr9+k8+eSTnDp1Cp1Ox6BBg0hNTWXlypWcPXuWqqoq/Pz80Ol0LF26lLCwsE6J/c9//pOXX36Zuro6LBYL/fr1w8nJiYiICE6dOsXFixe5ePEibm5u+Pn5dTjvf//73/z+97+nvr4eZ2dnEhISCAkJUV8vLi4mJiaGmpoa/Pz8mDVrFj//+c+ZPXs2JSUl9OnTh8GDB7Ns2bIO5dxW3KacGxoauHLlCoMGDcJqtTJ27FjOnj1rd855eXn85je/ARovr4+Pj8doNKqv19XVMW7cOOrq6rjrrruIiIhgxowZdufcVtymnK9du8b48eN577338Pb2JjExkWXLlvHZZ59htVp54IEHePnllzuUb5Nvv/2WqKgoFi1axLx585q99uSTT1JUVERGRgYzZsxQ92/ve7ut2NfnHBYWxrJlywD45ptvWLlyJeXl5Xh4ePDiiy9iMBha3e/y5cs5efIkbm5uAGzYsIH77rsPRVFYu3YtR48epW/fvmzYsIH7778fuPnvwRYUoTQ0NCgTJ05ULly4oNTV1SnR0dHKF1980eq2O3fuVJYuXarMnz9fbZs+fbryn//8R1EURdm7d6+SkpLSZrwJEyYoZWVlzdo2btyopKamKoqiKKmpqcrvfve7VvvaivXaa68py5cvV06ePKn8+9//VkaOHKlYrdY29/3FF18o0dHRSl1dnXLhwgVl4sSJSkNDQ4uYNTU1yvvvv6/s2bNHWb16dYvXMzMzlaVLlyrh4eFKfn6+EhUVpcTHxyv79+9Xtm7dqvzmN79Rnn322WZ97I1dXl6uhIWFKWVlZYrZbFbmz5+vHD9+XKmqqlJGjRqlvPzyy8rGjRuVlStXKs8+++wt5f3pp58qJSUliqIoyn//+1/lwQcfbPb6G2+8ocybN0+JiopSqqqqlEmTJilxcXHKggULlB07dijHjx9vlre9ca/P+dq1a8rSpUuV48ePKxaLRRkzZoyyefNmu3Oura1V6uvrFUVRFLPZrAQHB6v/rSiKcujQIWXx4sVKVFSUYrFYlNjYWGX27Nl252wr7vU5K0rj+3/OnDnK/Pnzlfj4eOVXv/qVkpqaqhw/flyZOnVqh/NtsmjRIiU+Pl7ZsWNHs/am93ZUVJT62R83blynvLdtxb4x52XLlinHjx9XFEVRP1eKoiiHDx9uEft6zz33nPL222+3aH/33XeVefPmKdeuXVM++ugjJTY2VlGUjn0P3kgObXHzS62UlJTw7rvvEhsb26z9/PnzjBkzBoBx48aRlZXV4THk5OQQExMDQExMDNnZ2a1uZyvWuXPnCA4OZsyYMdx11104OTmRn5/f5r5zcnKIiorCxcUFX19f/Pz8yMvLaxGzf//+jB49mj59+rR4raamhj/96U8sWLCAfv36qX+tffnll+pf0XfddVeL36e9sQsLC7n77rvx9PREr9cTGRlJZmYmAwYMQKPRMGTIEHJycli8eLGaf0fz/tGPfoS3d+PKq/7+/lgsFiwWi5r3/v37Wb58OQADBgxg2LBhnD9/Hl9fXwCCg4Ob5W1v3Otz1mg0jB8/nszMTBoaGrh69SojR460O+d+/fqh0zUeqKirq2t2g29NTQ1//vOfiY+PB6ChoYGGhga++eYbu3O2Fff6nEtKSvjuu+/o168f0PgeO3/+PDExMQQHB1NQUNDhfAGys7MZOnRoiys+r39v19fXq5/9mpqaTnlv24p9fc4AISEhZGZmqjk3xd6wYQMmk4kTJ060enO2LU3vDY1Gw6hRo7hy5QqlpaV2LTklhYTGGxmvnx56e3tjNrdcFnrdunUkJCTg5NT813bvvfeqv/BDhw5RXFzcbsx58+Yxffp0/v73vwNQVlam3kyp1+spLy9vtZ+tWCNGjCAnJ4eGhgaKi4upq6tTX7O175vNuy1btmzhl7/8JX379m3WPmLECPXN/+qrr1JTU8MzzzzD5cuXOyW2n58fX331FUVFRTQ0NJCTk0NJSQlFRUVYrVaKi4spKyvj448/pqamBmdnZ7vyzszM5L777sPFxaXVvIuKivj8888ZOXIk586dY/fu3YSHh1NTU8PXX3/dKXFvzDk7O5u33nqLsWPHMnToUAoLCzsl548//pioqCimTJnC6tWr1S/4ppydnZ35+uuvGTt2LGPHjmXUqFGdknNrca/Pec2aNXh5eVFRUQE0vsdKS0vR6/UcPnyY7777jrKysg7Fra2tZfv27SxatKjFa9f/G1+6dEn97A8YMKBT3tu2Ytt6bzfl3BT72WefxWq18qc//YnJkyfzyiuvtIiTkpJCdHQ069atU/8IunF8BoMBs9ls12dSCgk3t9TKO++8g6enJw888ECLbdeuXcuePXuYPn06NTU16peNLa+//joHDhxg+/bt7N69m/fff/+mx2or1owZMzAYDMyYMYM//vGP9O3bF61W2+a+bibvtnz++edcuHCh2TH0JsuWLeP9998nMzOTyMhI9Ho9Xl5ebNiwoVNiu7u7s2rVKp5++mn+53/+hyFDhqAoCosXL+b5558nLy+PmpoaTp48ibe3t/pleCuxv/jiC5KTk0lKSmo172vXrrF48WJWrlzJ888/j5OTE25ubuq5sG3btnVK3BtzHjp0KMHBwRw9epSBAwfyzjvvdErOgYGBmEwm9u3bR2pqKnV1dc1y1mq1+Pn5cfToUfLy8oiNje2UnFuL25TzvHnz+PDDD7nvvvvUP+SWLVuG1WolJiZGzbmj+W7bto0nnngCV1fXZu3X53vixAm0Wq362R8+fHinvLdtxW7tvd30WW76XMXExPDBBx/g7e3N73//e3bv3k1hYSETJkxQZz9Lly7l0KFDvPnmm1y+fFl9zIat8dnzmZST7bS+1IqnpydTp04F4NFHH+Xbb7/lyJEj5ObmUldXR3V1Nc8++yzJyckMHz6cnTt3Ao2Hnt5999024zW94QcNGoTRaCQvL49Bgwapf12Vlpaq09oVK1bw2Wefodfr2b59u81YOp2OlStXAo1/HUdFRXH33XercVrbt60lZg4fPswf/vAHANasWcPIkSNbzeOjjz4iPz+f8PBwGhoaKC8vZ+nSpWqOTfuoqakhOzubxx9/nCeffLJTYkPjRQtNKxns2bOHrKwsZs+ezaxZs5g1axYRERE8/vjjZGVl8d13391S3iUlJSxatIiNGzdy1113tci7vr6eixcvMnToUCZNmgTA9u3b1bwPHTrEmTNnOiXujTn//e9/x8nJiYEDB/Lggw/Sr18/iouL7c65yfDhw+nXrx9nz57lk08+afFvvXDhQn7605/y2WefdUrOrcUdOXIk4eHhfPTRR2RkZHDgwAHq6upQFIUXX3yRoUOHkpaWhqurK2+//TaDBg3qUNyPP/6YzMxMkpOTuXLlCk5OTvTp00c9NBweHk5FRQW1tbWMHDmSgQMHUl1djdFo5ODBg3a9t23Ffvzxx1v9d4aWn6tDhw7xz3/+k/3796PT6Vi7di0//OEPAdSjEC4uLkyfPl393rhxfCUlJej1eurr6299yambOpPyPVdfX6+Eh4c3O8l09uxZm9u/9957zU62X7p0SVEURbFarUpCQoKyd+9em31ramqUqqoq9edHHnlEOXr0qLJhw4ZmJ8Q3btzYan9bsWpra5WamhpFURQlPT1dCQgIUPvY2vfZs2ebnRQMDw9v86Tgm2++2erJdkVRlMLCQiUqKkr9/7KyMsVqtSpms1nZtGmTsnnzZuVPf/qTsmTJkk6L3fS7qKioUEaPHq0sW7ZMURRFjb1hwwZl9uzZyubNm28p78uXLyvR0dHKoUOHWh3TtWvXlIULFyo/+clP1LaysjKluLhYURRF2bRpkxIXF9fhnNuK25Tz+fPnlaioKOWrr75SvvvuO2XmzJlKdna23TlfuHBBPeldVFSkjBs3rtmFIWVlZcrnn3+uREVFKd99953y2GOPKRkZGXbn3FbcppwrKyuVKVOmKAcOHFDmz5+vlJWVqe/tTZs2KXPmzLnl97aiKMrWrVtbnGxXlP/33laUxs/+3LlzO/W93VrsG3P+6quv1N9/00U0UVFRyujRo5UXX3xROX/+fIt9ms1mRVEa36dr1qxRXnzxRUVRFOWdd95pdrJ9xowZiqJ0/HvwejIjwf6lVg4ePMiePXsAMBqNzJgxw+a2ZWVlLFy4EGi8lPjhhx8mNDSUkSNHsmTJEvbt24ePjw9btmzpUKyysjLmzZvHxYsXsVgsXLt2jdDQUOLj45k/f36r+/b39ycyMpLJkyej1WpJTEy0eTgsPDyc6upq6uvryc7OZufOnfzgBz9otk1xcTGPPvooFRUVPPTQQ2i1Wurq6tBqtXh7ezN06FD1ME1nxF67di1nzpzhu+++48qVK3z22WdMnTqVqqoq6uvr6dOnD1evXqW4uJjBgwd3OO/XXnuNCxcu8NJLL/HSSy8BsHPnTvWv3lOnTnH48GFcXFzU2WtoaCi7d+/GYrHQv39/goKCWLFiRafFbcq56a/yxYsXoygKP/jBD9i4cSPXrl2zK+dTp06xfft2dDodTk5OrFq1Sp3VAJSWlvLMM89w4cIFYmNjeeihh3BxcWHy5Ml25dxW3KacARYuXMidd94JwMmTJ8nKyuLSpUvodDruv/9+9XLVjry/OqqyspKHHnqIS5cu2f3etuXGnO+55x41502bNqHRaBg8eDBvvPEG/fv3b3Ufzz77LBUVFSiKwogRI1i9ejUAYWFhHD16FKPRSL9+/Vi3bh1g3/egLJEihBDCLnKyXQghhF2kkAghhLCLFBIhhBB2kUIihBDCLlJIhBBC2EUKiRAOsH//fvVy0K5QVFTEww8/3GXxhLieFBIhOoHVau3uIdiloaGhu4cgejC5IVH0atu3b6dPnz7MmTOHdevWcebMGf7yl79w4sQJ3nzzTZKTkzl48CCpqakoikJYWBgJCQkABAUFMXfuXP71r3/x3HPP8fXXX5OWloaXlxd33313q2uubdu2jW+//ZaioiK+/fZbnnjiCebMmUNRURFPPvkkBw8eBBoXA6ytrSU+Pp7Zs2dz33338emnn1JeXs7GjRtJS0vj7NmzREZG8vTTTwONxeC5557js88+45577mHjxo3069eP/Px8NmzYQG1tLXfccQfr169Hr9cze/ZsgoKC+PDDDwkPD8fHx4c//vGP6rpZu3fv7rp/CNGjyYxE9Gpjxozhgw8+ACA/P5/a2lrq6+s5deoUo0ePxmw2k5yczK5du0hPT+eTTz5Rlyqvra3F39+fvXv3ctddd7Ft2zZef/11du7cyblz52zGPH/+PK+++ip79+7lj3/8I/X19e2O09nZmd27d/Poo4/y1FNPkZiYyMGDBzlw4IC6Gu758+eZNWsWb731Fq6uruzZs4f6+nrWrFnD1q1b2b9/PzNmzCAlJUXd75UrV3jttdf45S9/yUsvvcSrr77KP/7xD15++WV7fq2il5FCInq1+++/n08//ZTq6mpcXFwYNWoU+fn5fPDBB4wePZpPPvmEn/zkJ3h6eqLT6YiOjlZXa9ZqtURERACNz7Rp2q5pyRBbwsLCcHFxwdPTE09PT3Xp87Y0LeB377334u/vj16vV5930bTQno+PDz/+8Y8BmDJlCqdOneL8+fOcPXuWuLg4pk6dyssvv9xsafDrxxkUFMTy5ct54403evyhOtG15NCW6NWcnZ0ZMmQI+/fvJygoiB/+8If85z//4cKFCwwfPpyCggKbffv06dNsDaWbXXL7+kNeWq2WhoYGdDod165dU9vr6upa7ePk5NSsv5OTk3p+48b4TUuD+/v7q8+9uVHTQ6IAkpKS+Pjjj3n33XeJiYkhPT2dO+6446ZyEr2bzEhErzdmzBh27tzJmDFjGD16NH/729+477770Gg0BAQE8P7771NeXo7VasVkMqlPqLxeQEAAJ0+epKKigvr6eg4dOtShMQwaNIiysjIqKiqwWCztPoqgNd9++y0fffQRACaTiR//+Mfcc889lJeXq+319fV88cUXrfa/cOECgYGB/N///R933HFHsyXFhWiLzEhErzd69GheeeUVRo0aRf/+/enTpw+jR48GGp/psHTpUp544gkURSE0NJSf//znLfah1+tZtGgRjz76KF5eXvzoRz9qNsNoj7OzMwsXLmTWrFkMHTqUYcOGdTiP4cOHc+DAARITE7n77rt57LHHcHFxYevWraxZs4aqqiqsVitPPPFEq6u6/u53v+Prr79GURSCg4MZMWJEh8cgeidZ/VcIIYRd5NCWEEIIu0ghEUIIYRcpJEIIIewihUQIIYRdpJAIIYSwixQSIYQQdpFCIoQQwi7/H60i92s8axflAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_order = [\"0-49\", \"50-99\", \"100-149\", \"150-199\", \"200-249\", \"250-299\", \"300-349\", \"350-399\", \"400-449\", \"450-499\",\">500\"]\n",
    "length_category = df['word_count'].apply(lambda x: length_order[min(10, int(x/50))])\n",
    "length_counts = pd.DataFrame(length_category.value_counts()).reset_index()\n",
    "length_counts.columns = ['word numbers', '# of moments']\n",
    "\n",
    "sns.barplot(x='word numbers', y='# of moments', data=length_counts, order=length_order)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "86063    * Fleury: Thanks businesses, local residents. ...\nName: body, dtype: object"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body'][df['word_count'] == 9324]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       ...,\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0]], dtype=uint8)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= pd.get_dummies(df['sentiment_category']).values\n",
    "num_classes = df['sentiment_category'].nunique()\n",
    "np.array(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#df['labels'] = df['']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# convert the original data to lowercase, strip and remove punctuations\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text=text.strip()\n",
    "    text=re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text\n",
    "\n",
    "# Lexicon - based\n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "\n",
    "# #2. STEMMING\n",
    "# # Initialize the stemmer\n",
    "# snow = SnowballStemmer('english')\n",
    "# def stemming(string):\n",
    "#     a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "#     return \" \".join(a)\n",
    "\n",
    "# 3.\n",
    "wl = WordNetLemmatizer()\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN# Tokenize the sentence\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xGvMVF2fBwOo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                body sentiment_category  \\\n0  The only comment I have is that people should ...                NEG   \n1  Honestly I just wish everyone would stop givin...                NEG   \n2  Yes. I’m starting to miss all those posts for ...                NEG   \n3  Imagine creating new accounts to feed anti vac...                NEG   \n4      I’m still good to complain about them, right?                NEU   \n\n   word_count                                         clean_text  \n0         502  comment people actually read protest trucker p...  \n1         174  honestly wish everyone would stop give loser a...  \n2          86  yes im start miss post every time bus get stic...  \n3          58   imagine create new account fee anti vacc garbage  \n4          45                       im still good complain right  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>sentiment_category</th>\n      <th>word_count</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The only comment I have is that people should ...</td>\n      <td>NEG</td>\n      <td>502</td>\n      <td>comment people actually read protest trucker p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Honestly I just wish everyone would stop givin...</td>\n      <td>NEG</td>\n      <td>174</td>\n      <td>honestly wish everyone would stop give loser a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Yes. I’m starting to miss all those posts for ...</td>\n      <td>NEG</td>\n      <td>86</td>\n      <td>yes im start miss post every time bus get stic...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Imagine creating new accounts to feed anti vac...</td>\n      <td>NEG</td>\n      <td>58</td>\n      <td>imagine create new account fee anti vacc garbage</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I’m still good to complain about them, right?</td>\n      <td>NEU</td>\n      <td>45</td>\n      <td>im still good complain right</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(clean_text(string)))\n",
    "df['clean_text'] = df['body'].apply(lambda x: finalpreprocess(x))\n",
    "df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RPOtTIucBwOp",
    "outputId": "73c7d32a-9ac6-49ea-efd2-a6b84fa71fb1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRAIN TEST SPLITTING OF LABELLED DATASET"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     body sentiment_category  \\\n0       The only comment I have is that people should ...                NEG   \n1       Honestly I just wish everyone would stop givin...                NEG   \n2       Yes. I’m starting to miss all those posts for ...                NEG   \n3       Imagine creating new accounts to feed anti vac...                NEG   \n4           I’m still good to complain about them, right?                NEU   \n...                                                   ...                ...   \n130152                            Just being neighbourly!                NEU   \n130153  I just recently heard that “sunshine” from 302...                NEU   \n130154  They are free to do this, its not like people ...                NEU   \n130155  I hope some of them do so they get charged. Wi...                NEG   \n130156                              Also actual pollution                NEG   \n\n        word_count                                         clean_text  \n0              502  comment people actually read protest trucker p...  \n1              174  honestly wish everyone would stop give loser a...  \n2               86  yes im start miss post every time bus get stic...  \n3               58   imagine create new account fee anti vacc garbage  \n4               45                       im still good complain right  \n...            ...                                                ...  \n130152          23                                        neighbourly  \n130153          94   recently hear sunshine coventry resign idea mean  \n130154         109            free like people object protest protest  \n130155         100            hope get charge luck cop stand checking  \n130156          21                              also actual pollution  \n\n[130156 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>sentiment_category</th>\n      <th>word_count</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The only comment I have is that people should ...</td>\n      <td>NEG</td>\n      <td>502</td>\n      <td>comment people actually read protest trucker p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Honestly I just wish everyone would stop givin...</td>\n      <td>NEG</td>\n      <td>174</td>\n      <td>honestly wish everyone would stop give loser a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Yes. I’m starting to miss all those posts for ...</td>\n      <td>NEG</td>\n      <td>86</td>\n      <td>yes im start miss post every time bus get stic...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Imagine creating new accounts to feed anti vac...</td>\n      <td>NEG</td>\n      <td>58</td>\n      <td>imagine create new account fee anti vacc garbage</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I’m still good to complain about them, right?</td>\n      <td>NEU</td>\n      <td>45</td>\n      <td>im still good complain right</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>130152</th>\n      <td>Just being neighbourly!</td>\n      <td>NEU</td>\n      <td>23</td>\n      <td>neighbourly</td>\n    </tr>\n    <tr>\n      <th>130153</th>\n      <td>I just recently heard that “sunshine” from 302...</td>\n      <td>NEU</td>\n      <td>94</td>\n      <td>recently hear sunshine coventry resign idea mean</td>\n    </tr>\n    <tr>\n      <th>130154</th>\n      <td>They are free to do this, its not like people ...</td>\n      <td>NEU</td>\n      <td>109</td>\n      <td>free like people object protest protest</td>\n    </tr>\n    <tr>\n      <th>130155</th>\n      <td>I hope some of them do so they get charged. Wi...</td>\n      <td>NEG</td>\n      <td>100</td>\n      <td>hope get charge luck cop stand checking</td>\n    </tr>\n    <tr>\n      <th>130156</th>\n      <td>Also actual pollution</td>\n      <td>NEG</td>\n      <td>21</td>\n      <td>also actual pollution</td>\n    </tr>\n  </tbody>\n</table>\n<p>130156 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83299 26032 20825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, x_test, Y_train, y_test = train_test_split(df[\"clean_text\"], y, test_size=0.2,shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2,shuffle=True)\n",
    "\n",
    "print(len(x_train),len(x_test),len(x_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3). Logistic Regression with Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "1FmdwhShBwOv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenized with Bert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "AsbEQjVkBwOw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)#, model_max_length=512)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cJxf5V8EBwOx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1016 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (825 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1182 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1322 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (824 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1288 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1224 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1014 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1318 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1299 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1971 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (935 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1060 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (824 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (889 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  1973\n"
     ]
    }
   ],
   "source": [
    "# Concatenate train data and test data\n",
    "all_text = df.body.values\n",
    "# Encode our concatenated data\n",
    "encoded_text = [tokenizer.encode(i, add_special_tokens=True) for i in all_text]\n",
    "#print(encoded_text)\n",
    "embed_vocab = {}\n",
    "for word in tokenizer.vocab.keys():\n",
    "    embed_vocab[word] = True\n",
    "#print(len(embed_vocab))\n",
    "# Find the maximum length\n",
    "max_len = max([len(i) for i in encoded_text])\n",
    "print('Max length: ', max_len)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cDfSQ6qQBwOx",
    "outputId": "ebb5f830-602c-4f3e-950c-4d0726d12fdd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "text = df['clean_text']\n",
    "text_words = []\n",
    "text_vocab = {}\n",
    "for string in text:\n",
    "    # Don't include words that can't actually be mapped to a vector using embeddings\n",
    "    words = string.split()\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        text_vocab[word] = True\n",
    "        if embed_vocab.get(word) is not None:\n",
    "            filtered_words.append(word)\n",
    "    text_words.append(filtered_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# print(text_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12785 words were found\n",
      "23143 words weren't found\n"
     ]
    }
   ],
   "source": [
    "def vocab_coverage(compared, base):  #, k):\n",
    "    hit, miss = 0, 0\n",
    "    for word in compared:\n",
    "        if base.get(word) is not None:\n",
    "            hit += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "            #base[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "    print(\"{} words were found\".format(hit))\n",
    "    print(\"{} words weren't found\".format(miss))\n",
    "    #print(\"new length of {} word_vectors\".format(len(embed_vocab)))\n",
    "\n",
    "vocab_coverage(text_vocab, embed_vocab)  # , 300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def preprocessing_data(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # For every sentence...\n",
    "    for i in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = i ,  # Preprocess sentence\n",
    "            add_special_tokens=False,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=False      # Return attention mask\n",
    "            )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "\n",
    "    # # Convert lists to tensors\n",
    "    # input_ids = torch.tensor(input_ids)\n",
    "    # attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8xlJ2nf3BwOy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "dHEjc6hgBwOy",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#prepare input under agency/sociality/combined class\n",
    "train_inputs = preprocessing_data(x_train)\n",
    "val_inputs = preprocessing_data(x_val)\n",
    "test_inputs = preprocessing_data(x_test)\n",
    "#\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "LzMzVcKRBwOy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "17b828b1-5644-442f-e40e-0a9d2d41165a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# test_inputs = preprocessing_data(df['clean_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basic classifiers under Bert tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Pg2XMPgCBwOz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x7f15f8da5790>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "print(type(y_train))\n",
    "\n",
    "datasets = {\n",
    "    'train': TensorDataset(torch.tensor(train_inputs), torch.tensor(y_train)),\n",
    "    'val': TensorDataset(torch.tensor(val_inputs),  torch.tensor(y_val)),\n",
    "    'test': TensorDataset(torch.tensor(test_inputs),  torch.tensor(y_test)),\n",
    "\n",
    "}\n",
    "print(datasets['train'])\n",
    "dataloaders = {\n",
    "    'train': DataLoader(datasets['train'], batch_size=batch_size),\n",
    "    'val': DataLoader(datasets['val'], batch_size=batch_size),\n",
    "    'test': DataLoader(datasets['test'], batch_size=batch_size),\n",
    "}"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gNeS39X_BwO0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. [30 marks] Train an advanced classifier based on deep learning, such as fine-tuning BERT. You can try predicting both labels independently, or in a multi-task manner. Use part of the training data for validation, and keep aside the test data when choosing the best model.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "cE1MUmYaBwO1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "l8q6XsqaBwO2",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on the GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the CPU\")\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "O6sI2YXABwO2",
    "outputId": "91e14c35-1a01-403a-96b2-e1a216b9c60a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ch4CEVv6BwO4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.895622   |  0.869465  |   0.67   \n",
      "   2    |    -    |   0.826196   |  0.859147  |   0.68   \n",
      "   3    |    -    |   0.796596   |  0.861152  |   0.68   \n",
      "   4    |    -    |   0.797327   |  0.858922  |   0.68   \n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model that will be used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim, num_filters=100, kernel_sizes=[2, 4, 7], freeze_embeddings=True, drop_prob=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        # set class vars\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained\n",
    "        #self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
    "        # (optional) freeze embedding weights\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "\n",
    "        # 2. convolutional layers\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2, 0))\n",
    "            for k in kernel_sizes])\n",
    "\n",
    "        # 3. final, fully-connected layer for classification\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
    "\n",
    "        # 4. dropout and sigmoid layers\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.act = nn.Softmax(dim=-1)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional + max pooling layer\n",
    "        \"\"\"\n",
    "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        # conv_seq_length will be ~ 500\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "\n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how a batch of inputs, x, passes through the model layers.\n",
    "        Returns a single, sigmoid-activated class score as output.\n",
    "        \"\"\"\n",
    "        # embedded vectors\n",
    "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
    "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "\n",
    "        # get output of each conv-pool layer\n",
    "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
    "\n",
    "        # concatenate results and add dropout\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # final logit\n",
    "        logit = self.fc(x)\n",
    "\n",
    "        # # sigmoid-activated --> a class score\n",
    "        output = self.act(logit)\n",
    "\n",
    "        return output\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "def initialize_model(device, epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate model\n",
    "    vocab_size = len(embed_vocab)\n",
    "    output_size = 3\n",
    "    embedding_dim = 300\n",
    "    num_filters = 100\n",
    "    kernel_sizes = [2, 4, 7]\n",
    "\n",
    "    net = SentimentCNN(tokenizer, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    if device:\n",
    "        net.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "    # Create the optimizer\n",
    "    lr=0.001\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(dataloaders['train']) * epochs\n",
    "\n",
    "    # # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return net, optimizer, scheduler\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    # nonacc = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        inputs, labels = tuple(t.to(torch.device(\"cuda:0\")) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.round(logits)\n",
    "        p = (preds - labels).cpu().numpy()\n",
    "        num = np.count_nonzero(p, axis = 1)\n",
    "        zero_num = num == 0\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = zero_num.mean()\n",
    "        #accuracy =\n",
    "        # non = (preds != labels).cpu().numpy()\n",
    "        # nonacc.append(non)\n",
    "        #print(\"acc:\",accuracy)\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    # print(nonacc)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# training loop\n",
    "def train(model, train_loader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    device=torch.cuda.is_available()\n",
    "    if(device):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0 # for printing\n",
    "    # Print the header of the result table\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} \")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    #min_loss = np.Inf\n",
    "    # train for some number of epochs\n",
    "    #net.train()\n",
    "    for i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        total_loss, batch_loss = 0, 0\n",
    "        net.train()\n",
    "\n",
    "        # batch loop\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            counter +=1\n",
    "            # Load batch to GPU\n",
    "            if(device):\n",
    "                inputs, labels = tuple(t.to(torch.device(\"cuda:0\")) for t in batch)\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "            #y = torch.softmax(output, dim=1)\n",
    "\n",
    "            loss = criterion(output, labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "             # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "\n",
    "\n",
    "            print(f\"{i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f}\")\n",
    "\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "net, optimizer,scheduler = initialize_model(device, epochs=2)\n",
    "train(net, dataloaders['train'], dataloaders['val'], epochs=4, evaluation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# torch.cuda.memory_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "Xdyv6DLdBwO5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-22499ce0d343>:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.act(logit)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-3812fb556bc9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0;31m# Compute predicted probabilities on the test set\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m \u001B[0mprobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_accuracy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcnn_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataloaders\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'test'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Acc: {test_accuracy:^.2f}, Loss: {test_loss:^10.6f} '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0mc_report\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclassification_report\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'NEU'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'NEG'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'POS'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-32-3812fb556bc9>\u001B[0m in \u001B[0;36mcnn_predict\u001B[0;34m(model, test_dataloader)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;31m#pred = torch.round(logits)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0mall_labels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m         \u001B[0mall_logits\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;31m# Compute loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cnn_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        if(device):\n",
    "            inputs, labels = tuple(t.to(torch.device(\"cuda:0\")) for t in batch)\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)\n",
    "        #pred = torch.round(logits)\n",
    "        all_labels.append(labels)\n",
    "        all_logits.append(pred)\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels.float())\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.round(logits)\n",
    "\n",
    "        p = (preds - labels).cpu().numpy()\n",
    "        num = np.count_nonzero(p, axis = 1)\n",
    "        zero_num = num == 0\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = zero_num.mean()\n",
    "        #print(\"test:\", accuracy)\n",
    "        test_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    all_logits = all_logits.to(\"cpu\").detach().numpy()\n",
    "    all_labels = all_labels.to(\"cpu\").detach().numpy()\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return all_logits,all_labels, test_loss, test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Compute predicted probabilities on the test set\n",
    "probs, labels, test_loss, test_accuracy = cnn_predict(net, dataloaders['test'])\n",
    "print(f'Acc: {test_accuracy:^.2f}, Loss: {test_loss:^10.6f} ')\n",
    "c_report = classification_report(labels, probs, target_names=['NEU', 'NEG', 'POS'])\n",
    "print(c_report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(type(probs))\n",
    "# print(type(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(type(probs.astype(int)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = np.argmax(probs, axis=1)\n",
    "# label\n",
    "y_train = np.argmax(labels, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def get_confusion_matrix(trues, preds):\n",
    "  labels = [0,1,2]\n",
    "  conf_matrix = confusion_matrix(y_true= trues, y_pred = preds, labels = labels)\n",
    "  return conf_matrix\n",
    "def plot_confusion_matrix(conf_matrix):\n",
    "  plt.imshow(conf_matrix, cmap=plt.cm.Blues_r)\n",
    "  indices = range(conf_matrix.shape[0])\n",
    "  labels = [0, 1,2]\n",
    "  plt.xticks(indices, labels)\n",
    "  plt.yticks(indices, labels)\n",
    "  plt.colorbar()\n",
    "  plt.xlabel('y_pred')\n",
    "  plt.ylabel('y_true')\n",
    "  # 显示数据\n",
    "  for first_index in range(conf_matrix.shape[0]):\n",
    "    for second_index in range(conf_matrix.shape[1]):\n",
    "      plt.text(first_index, second_index, conf_matrix[first_index, second_index])\n",
    "  plt.savefig('heatmap_confusion_matrix.jpg')\n",
    "  plt.show()\n",
    "\n",
    "conf_matrix = get_confusion_matrix(y_train, pred)\n",
    "print(conf_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs = list(probs)\n",
    "\n",
    "def multi_indicator(probs):\n",
    "    int_probs = []\n",
    "    probs_i = []\n",
    "    for i in probs:\n",
    "        for j in i:\n",
    "            j =int(j)\n",
    "            int_probs.append(j)\n",
    "        probs_i.append(int_probs)\n",
    "        int_probs = []\n",
    "    return probs_i\n",
    "\n",
    "probs_i = multi_indicator(probs)\n",
    "probs_i\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = multi_indicator(labels)\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "def plot_roc(y_score, y_label):\n",
    "    n_classes = len(y_score[0])\n",
    "\n",
    "    # 计算每一类的ROC\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_label[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # micro\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_label.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # macro\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    lw=2\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('multi-calss ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_roc(np.array(probs_i), np.array(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-282d5b2f",
   "language": "python",
   "display_name": "PyCharm (NLP_Assignments)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "A2.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}